# 5_Gradient Descent

## 1 基本概念介绍

##### 梯度下降法

不是一个机器学习算法，而是一种基于搜索的最优化方法。作用是对原始模型的损失函数进行优化，以便寻找到最优参数，使得损失函数的值最小。

优点：若随机试错，效率太低；从损失值出发更新参数，可以大幅度降低计算次数。通过导数告诉我们参数的运行方向，怎样运用能安全高效降低损失值。

##### 梯度

多元函数的导数就是梯度。梯度其实是一个向量，由损失函数的偏导构成。这个变量指出了函数在给定点上升最快的方向。

梯度指向误差值增加最快的方向，导数为0的点，就是优化问题的解。

沿着梯度反方向进行线性搜索，从而减少误差值。每次搜索的步长为某个特定的数值，直到梯度与0向量非常接近。

## 2 模型介绍

一句话概括：首先随机选择一个方向，每次迈步都选择最陡的方向，直到这个方向上能达到的最低点。

步长影响算法正确性和效率

- 太大则泰勒公式不准确，可能会迈过最低点，从而发生摇摆的现象
- 太小会导致每次迭代参数基本不变化，收敛速度变慢

（数学推导部分还需要再耐下心来好好看一下）

## 3 模型缺点

理论上只能保证找到局部最优，而非全局最优。

解决方法：先产生多个初始值，同时开始迭代，在最终结果中选择最好的那一个。

## 4 算法实现

### 4.1 求导

- spicy.misc.derivative(func, x0, dx=1.0, n=1, args=(), order=3)[source]
- sympy 符号化运算库

（算法实现自己有空要自己打一遍代码）

## 1 算法概览

梯度下降是目前机器学习、深度学习解决最优化问题的算法中，最核心、应用最广的方法。

GD不是一个机器学习算法，而是一种基于搜索的优化算法。作用是对原始模型损失函数进行优化，找到使损失函数最小的最优参数。

从损失值出发，更新参数，大幅降低计算次数。

## 2 梯度

定义：多元函数的导数就是梯度，对每个变量进行微分，然后形成一个向量。（也是函数）

单变量函数中，梯度代表函数在某个定点切线的斜率。多变量函数中，梯度是一个向量，指出了函数在给定点上升最快的方向。

梯度指向误差值增加最快的方向，导数为0的点，就是优化问题的解。

为了找到解，我们沿着梯度的反方向进行线性搜索，从而减少误差值。

## 3 梯度下降法

类比：从山顶下山，找一条最近的下山的路，每走一步都判断下，下去最快的方向是哪一个。

**关于学习率**

- 太大时不准确。可能迈过最低点，出现摇摆现象（不收敛）。
- 太小收敛慢。

**核心问题**

只能找到局部最优。

**优化方案**

生成多个初始值，同时进行迭代。

## 4 SGD

批量梯度下降法每次对参数进行更新的时候，使用的是所有样本计算出来的梯度。

为了提高计算速度，SGD每次只使用一个点来计算向量的搜索方向。

虽然每次不是沿着下降最快的方向进行，但是每次的计算速度显著加快。

**步长很重要**

若步长取固定值，则收敛到最优解附近后，又会跳去其他地方，误差增大。

所以步长一般随学习次数增加而下降，一般是学习次数的倒数。



























