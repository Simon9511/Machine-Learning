# 5_Gradient Descent

## 1 基本概念介绍

##### 梯度下降法

不是一个机器学习算法，而是一种基于搜索的最优化方法。作用是对原始模型的损失函数进行优化，以便寻找到最优参数，使得损失函数的值最小。

优点：若随机试错，效率太低；从损失值出发更新参数，可以大幅度降低计算次数。通过导数告诉我们参数的运行方向，怎样运用能安全高效降低损失值。

##### 梯度

多元函数的导数就是梯度。梯度其实是一个向量，由损失函数的偏导构成。这个变量指出了函数在给定点上升最快的方向。

梯度指向误差值增加最快的方向，导数为0的点，就是优化问题的解。

沿着梯度反方向进行线性搜索，从而减少误差值。每次搜索的步长为某个特定的数值，直到梯度与0向量非常接近。

## 2 模型介绍

一句话概括：首先随机选择一个方向，每次迈步都选择最陡的方向，直到这个方向上能达到的最低点。

步长影响算法正确性和效率

- 太大则泰勒公式不准确，可能会迈过最低点，从而发生摇摆的现象
- 太小会导致每次迭代参数基本不变化，收敛速度变慢

（数学推导部分还需要再耐下心来好好看一下）

## 3 模型缺点

理论上只能保证找到局部最优，而非全局最优。

解决方法：先产生多个初始值，同时开始迭代，在最终结果中选择最好的那一个。

## 4 算法实现

### 4.1 求导

- spicy.misc.derivative(func, x0, dx=1.0, n=1, args=(), order=3)[source]
- sympy 符号化运算库

（算法实现自己有空要自己打一遍代码）





























